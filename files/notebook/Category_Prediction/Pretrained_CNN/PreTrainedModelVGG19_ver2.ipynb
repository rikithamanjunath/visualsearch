{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Reference: https://www.kaggle.com/dansbecker/transfer-learning\n",
    "# importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import keras\n",
    "\n",
    "from glob import glob\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.applications.vgg19 import VGG19\n",
    "import tensorflow as tf\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,MaxPool2D,Flatten,Dense,Dropout\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chiffon', 'Jeans', 'Lace_Dress', \"Men's_Shirts\", 'Skirt']\n"
     ]
    }
   ],
   "source": [
    "# loading the directories \n",
    "training_dir = 'C:/Users/avadh/Desktop/CMPE 295A/Dataset/Temp/Semi_Train'\n",
    "test_dir = 'C:/Users/avadh/Desktop/CMPE 295A/Dataset/Temp/Semi_Test'\n",
    "folders=os.listdir(\"C:/Users/avadh/Desktop/CMPE 295A/Dataset/Temp/Semi_Train\")\n",
    "print(folders)\n",
    "#test_dir = '../input/fruits/fruits-360_dataset/fruits-360/test-multiple_fruits/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=64,64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful for getting number of files\n",
    "image_files = glob(training_dir + '/*/*.jp*g')\n",
    "test_image_files = glob(test_dir + '/*/*.jp*g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_train=[]\n",
    "image_labels=[]\n",
    "image_names=[]\n",
    "\n",
    "for folder in folders:\n",
    "    for each in os.listdir(os.path.join(training_dir,folder)):\n",
    "        if each.endswith('jpg'):\n",
    "            image_names.append(os.path.join(training_dir,folder,each))\n",
    "            image_labels.append(folder)\n",
    "            img=cv2.imread(os.path.join(training_dir,folder,each))\n",
    "            img_in=cv2.resize(img,size)\n",
    "            image_train.append(img_in)\n",
    "        else:\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_list=list(zip(image_train,image_labels))\n",
    "random.shuffle(union_list)\n",
    "train,labels=zip(*union_list)\n",
    "X=np.array(train)\n",
    "Y=np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X/255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "le=LabelEncoder()\n",
    "Y=le.fit_transform(Y)\n",
    "Y=to_categorical(Y,num_classes=len(folders))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_val,Y_train,Y_val=train_test_split(X,Y,test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19_model=VGG19(include_top=False,input_shape=(64,64,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "\n",
    "for layers in vgg19_model.layers:\n",
    "    model.add(layers)\n",
    "    \n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4096,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "for layer in vgg19_model.layers:\n",
    "    layer.trainable=False\n",
    "\n",
    "for layers in vgg19_model.layers[-2:]:\n",
    "    layers.trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(len(folders),activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "block1_conv1 (Conv2D)        (None, 64, 64, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 64, 64, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 32, 32, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 8, 8, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              8392704   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 20485     \n",
      "=================================================================\n",
      "Total params: 45,218,885\n",
      "Trainable params: 27,554,309\n",
      "Non-trainable params: 17,664,576\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=RMSprop(lr=0.0001)   #,rho=0.9,epsilon=1e-08,decay=0.0)\n",
    "model.compile(optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "learning_rate_reduction=ReduceLROnPlateau(monitor='val_acc',patience=3,verbose=1,factor=0.5,minlr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  \n",
    "        samplewise_center=False,  \n",
    "        featurewise_std_normalization=False,  \n",
    "        samplewise_std_normalization=False,  \n",
    "        zca_whitening=False,\n",
    "        rotation_range=10,  \n",
    "        zoom_range = 0.1, \n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,  \n",
    "        horizontal_flip=False,\n",
    "        vertical_flip=False) \n",
    "\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "536/535 [==============================] - 777s 1s/step - loss: 1.1685 - acc: 0.5368 - val_loss: 0.9767 - val_acc: 0.5872\n",
      "Epoch 2/5\n",
      "536/535 [==============================] - 628s 1s/step - loss: 0.9810 - acc: 0.6267 - val_loss: 0.8623 - val_acc: 0.6678\n",
      "Epoch 3/5\n",
      "536/535 [==============================] - 594s 1s/step - loss: 0.9098 - acc: 0.6556 - val_loss: 0.9167 - val_acc: 0.6577\n",
      "Epoch 4/5\n",
      "536/535 [==============================] - 587s 1s/step - loss: 0.8732 - acc: 0.6799 - val_loss: 0.8496 - val_acc: 0.7013\n",
      "Epoch 5/5\n",
      "449/535 [========================>.....] - ETA: 1:32 - loss: 0.8282 - acc: 0.6987"
     ]
    }
   ],
   "source": [
    "history=model.fit_generator(datagen.flow(X_train,Y_train,batch_size=10),epochs=5,validation_data=(X_val,Y_val),\n",
    "                          verbose=1,steps_per_epoch=X_train.shape[0]/10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5b663228bd00>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# serialize model to JSON\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmodel_json\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Dataset/MyVersion/VGG16.json\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mjson_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_json\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#Reference: https://machinelearningmastery.com/save-load-keras-deep-learning-models/\n",
    "\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"Dataset/MyVersion/VGG16.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # getting the number of classes i.e. type of fruits\n",
    "# folders = glob(training_dir + '/*')\n",
    "# num_classes = len(folders)\n",
    "# print ('Total Classes = ' + str(num_classes))\n",
    "\n",
    "# size=64,64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # importing the libraries\n",
    "# from keras.models import Model\n",
    "# from keras.layers import Flatten, Dense\n",
    "# from keras.applications import VGG16\n",
    "# from keras.applications.vgg19 import VGG19\n",
    "# #from keras.preprocessing import image\n",
    "\n",
    "# IMAGE_SIZE = [64, 64]  # we will keep the image size as (64,64). You can increase the size for better results. \n",
    "\n",
    "# # loading the weights of VGG16 without the top layer. These weights are trained on Imagenet dataset.\n",
    "# vgg = VGG16(input_shape = IMAGE_SIZE + [3], weights = 'imagenet', include_top = False)  # input_shape = (64,64,3) as required by VGG\n",
    "\n",
    "# # this will exclude the initial layers from training phase as there are already been trained.\n",
    "# for layer in vgg.layers:\n",
    "#     layer.trainable = False\n",
    "\n",
    "# x = Flatten()(vgg.output)\n",
    "# #x = Dense(128, activation = 'relu')(x)   # we can add a new fully connected layer but it will increase the execution time.\n",
    "# x = Dense(num_classes, activation = 'softmax')(x)  # adding the output layer with softmax function as this is a multi label classification problem.\n",
    "\n",
    "# model = Model(inputs = vgg.input, outputs = x)\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Image Augmentation\n",
    "\n",
    "# from keras.preprocessing.image import ImageDataGenerator\n",
    "# from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# training_datagen = ImageDataGenerator(\n",
    "#                                     rescale=1./255,   # all pixel values will be between 0 an 1\n",
    "#                                     shear_range=0.2, \n",
    "#                                     zoom_range=0.2,\n",
    "#                                     horizontal_flip=True,\n",
    "#                                     preprocessing_function=preprocess_input)\n",
    "\n",
    "# # validation_datagen = ImageDataGenerator(rescale = 1./255, preprocessing_function=preprocess_input)\n",
    "\n",
    "# test_datagen = ImageDataGenerator(\n",
    "#                                     rescale=1./255,   # all pixel values will be between 0 an 1\n",
    "#                                     preprocessing_function=preprocess_input)\n",
    "\n",
    "\n",
    "# training_generator = training_datagen.flow_from_directory(training_dir, target_size = IMAGE_SIZE, batch_size = 200, class_mode = 'categorical')\n",
    "# test_generator = test_datagen.flow_from_directory(test_dir, target_size = IMAGE_SIZE, batch_size = 200, class_mode= 'categorical')\n",
    "\n",
    "# #validation_generator = validation_datagen.flow_from_directory(validation_dir, target_size = IMAGE_SIZE, batch_size = 200, class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The labels are stored in class_indices in dictionary form. \n",
    "# # checking the labels\n",
    "# training_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #training_images = 15000\n",
    "# #validation_images = 12709\n",
    "\n",
    "# history = model.fit_generator(training_generator,\n",
    "#                               steps_per_epoch = 201,  # this should be equal to total number of images in training set. But to speed up the execution, I am only using 10000 images. Change this for better results. \n",
    "#                               epochs = 3,\n",
    "#                               validation_data = test_generator,\n",
    "#                               validation_steps= 59)  # change this for better results\n",
    "# #                    ,\n",
    "# #                    validation_steps = 3000)  # this should be equal to total number of images in validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print ('Training Accuracy = ' + str(history.history['acc']))\n",
    "# #print ('Validation Accuracy = ' + str(history.history['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.preprocessing import image;\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = image.load_img('Dataset/img/Blouse/id_00000731/shop_01.jpg',target_size=(64,64));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = image.img_to_array(temp)\n",
    "# temp = np.expand_dims(temp, axis=0)\n",
    "# result = model.predict(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Reference: https://machinelearningmastery.com/save-load-keras-deep-learning-models/\n",
    "\n",
    "# from keras.models import model_from_json\n",
    "\n",
    "# # serialize model to JSON\n",
    "# model_json = model.to_json()\n",
    "# with open(\"Dataset/MyVersion/VGG16.json\", \"w\") as json_file:\n",
    "#     json_file.write(model_json)\n",
    "    \n",
    "# # serialize weights to HDF5\n",
    "# model.save_weights(\"model.h5\")\n",
    "# print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pwd\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
