{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,MaxPool2D,Flatten,Dense,Dropout\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Training Set\n",
    "\n",
    "data_trial = \"/Users/vineevineela/Downloads/Consumer_To_Shop/Train\"\n",
    "IMG_SIZE = 64\n",
    "NO_OF_CATEGORIES = 3\n",
    "train_image = []\n",
    "train_label = []\n",
    "\n",
    "\n",
    "#enters folders (dress,shirt,pants) in train\n",
    "for folder in os.listdir(data_trial):\n",
    "    lab = folder\n",
    "    if('.DS_Store' not in folder):\n",
    "        \n",
    "        #sub_folder is train/dress\n",
    "        sub_folder = os.path.join(data_trial,folder)\n",
    "        if('.DS_Store' not in sub_folder):\n",
    "             for img in os.listdir(sub_folder):\n",
    "                    path = os.path.join(sub_folder,img)\n",
    "                    if('.DS_Store' not in path):\n",
    "                        img = cv2.imread(path,cv2.IMREAD_COLOR)\n",
    "                        img = cv2.resize(img,(IMG_SIZE,IMG_SIZE))\n",
    "                        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "                        train_image.append(img)\n",
    "                        train_label.append(lab)\n",
    "                        \n",
    "train_image = np.array(train_image)\n",
    "train_label = np.array(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Test Set\n",
    "\n",
    "data_trial = \"/Users/vineevineela/Downloads/Consumer_To_Shop/Test\"\n",
    "IMG_SIZE = 64\n",
    "test_image = []\n",
    "test_label = []\n",
    "\n",
    "#enters folders (dress,shirt,pants) in train\n",
    "for folder in os.listdir(data_trial):\n",
    "    lab = folder\n",
    "    if('.DS_Store' not in folder):\n",
    "        \n",
    "        #sub_folder is train/dress\n",
    "        sub_folder = os.path.join(data_trial,folder)\n",
    "        if('.DS_Store' not in sub_folder):\n",
    "             for img in os.listdir(sub_folder):\n",
    "                    path = os.path.join(sub_folder,img)\n",
    "                    if('.DS_Store' not in path):\n",
    "                        img = cv2.imread(path,cv2.IMREAD_COLOR)\n",
    "                        img = cv2.resize(img,(IMG_SIZE,IMG_SIZE))\n",
    "                        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "                        test_image.append(img)\n",
    "                        test_label.append(lab)\n",
    "                        \n",
    "test_image = np.array(test_image)\n",
    "test_label = np.array(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image = train_image/255\n",
    "test_image = test_image/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "le=LabelEncoder()\n",
    "train_label=le.fit_transform(train_label)\n",
    "train_label=to_categorical(train_label,num_classes=NO_OF_CATEGORIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "le=LabelEncoder()\n",
    "test_label=le.fit_transform(test_label)\n",
    "test_label=to_categorical(test_label,num_classes=NO_OF_CATEGORIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_model=keras.applications.vgg16.VGG16(include_top=False,input_shape=(64,64,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layers in vgg16_model.layers:\n",
    "    model.add(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())\n",
    "model.add(Dense(4096,activation='relu'))\n",
    "model.add(Dense(4096,activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(NO_OF_CATEGORIES,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "block1_conv1 (Conv2D)        (None, 64, 64, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 64, 64, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 32, 32, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 8, 8, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              8392704   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 12291     \n",
      "=================================================================\n",
      "Total params: 39,900,995\n",
      "Trainable params: 12,291\n",
      "Non-trainable params: 39,888,704\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=RMSprop(lr=0.001,rho=0.9,epsilon=1e-08,decay=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer,loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_reduction=ReduceLROnPlateau(monitor='val_acc',patience=3,verbose=1,factor=0.5,minlr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  \n",
    "        samplewise_center=False,  \n",
    "        featurewise_std_normalization=False,  \n",
    "        samplewise_std_normalization=False,  \n",
    "        zca_whitening=False,\n",
    "        rotation_range=10,  \n",
    "        zoom_range = 0.1, \n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,  \n",
    "        horizontal_flip=False,\n",
    "        vertical_flip=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen.fit(train_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "4/3 [=================================] - 106s 26s/step - loss: 1.1250 - acc: 0.5133 - val_loss: 0.7563 - val_acc: 0.6667\n",
      "Epoch 2/5\n",
      "4/3 [=================================] - 101s 25s/step - loss: 0.7898 - acc: 0.6800 - val_loss: 0.8811 - val_acc: 0.6667\n",
      "Epoch 3/5\n",
      "4/3 [=================================] - 96s 24s/step - loss: 0.8646 - acc: 0.6667 - val_loss: 0.8809 - val_acc: 0.6667\n",
      "Epoch 4/5\n",
      "4/3 [=================================] - 108s 27s/step - loss: 0.7356 - acc: 0.6667 - val_loss: 0.7484 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 5/5\n",
      "4/3 [=================================] - 110s 28s/step - loss: 0.6877 - acc: 0.6667 - val_loss: 0.7287 - val_acc: 0.6667\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(datagen.flow(train_image,train_label,batch_size=10),epochs=5,validation_data=(test_image,test_label),\n",
    "                          verbose=1,steps_per_epoch=train_image.shape[0]/10, callbacks=[learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(test_image, test_label, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.728740930557251\n",
      "Test accuracy: 0.6666666865348816\n"
     ]
    }
   ],
   "source": [
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
